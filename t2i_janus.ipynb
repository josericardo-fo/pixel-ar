{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55837fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heinz/josericardo/models/Janus/.venv/lib/python3.12/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heinz/josericardo/models/Janus/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add image tag = <image_placeholder> to the tokenizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "\n",
    "# Caminho do modelo\n",
    "model_path = \"deepseek-ai/Janus-1.3B\"\n",
    "\n",
    "# Carrega o processor (tokenizer + formatação de chat)\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "# Carrega o modelo multimodal\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Envia para GPU em bfloat16 (ajuste se necessário)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b963ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Você pode editar esse texto quando quiser testar outros prompts\n",
    "prompt_text = (\n",
    "    \"A realistic stone statue of a male human figure, standing upright, carved from aged grey rock. \"\n",
    "    \"Detailed masculine facial features, smooth yet weathered surface texture, soft natural lighting, \"\n",
    "    \"neutral background. Emphasize lifelike proportions, fine chiseling marks, and a calm, serene expression. \"\n",
    "    \"High resolution.\"\n",
    ")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": prompt_text,\n",
    "    },\n",
    "    {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# Aplica o template de SFT (formato de conversa) e adiciona a tag de imagem\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"\",\n",
    ")\n",
    "prompt = sft_format + vl_chat_processor.image_start_tag\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    parallel_size: int = 16,\n",
    "    cfg_weight: float = 5.0,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "    output_dir: str = \"generated_samples\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera imagens e salva em `output_dir` como img_0.jpg, img_1.jpg, ...\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokeniza o prompt\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "    # Constrói batch com condicionais e incondicionais (para CFG)\n",
    "    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).cuda()\n",
    "    for i in range(parallel_size * 2):\n",
    "        tokens[i, :] = input_ids\n",
    "        if i % 2 != 0:\n",
    "            # Linhas ímpares viram \"unconditional\" com padding\n",
    "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    # Embeddings de entrada\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    # Buffer para tokens de imagem gerados\n",
    "    generated_tokens = torch.zeros(\n",
    "        (parallel_size, image_token_num_per_image), dtype=torch.int\n",
    "    ).cuda()\n",
    "\n",
    "    outputs = None\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = mmgpt.language_model.model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=True,\n",
    "            past_key_values=outputs.past_key_values if i != 0 else None,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Cabeça de geração de imagem\n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "\n",
    "        # Classifier-free guidance\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        # Amostra próximo token\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        # Prepara embeddings de imagem para próximo passo\n",
    "        next_token = torch.cat(\n",
    "            [next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1\n",
    "        ).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "    # Decodifica códigos de imagem\n",
    "    dec = mmgpt.gen_vision_model.decode_code(\n",
    "        generated_tokens.to(dtype=torch.int),\n",
    "        shape=[parallel_size, 8, img_size // patch_size, img_size // patch_size],\n",
    "    )\n",
    "\n",
    "    # Converte para uint8 [0, 255]\n",
    "    dec = (\n",
    "        dec.to(torch.float32)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "        .transpose(0, 2, 3, 1)\n",
    "    )\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "\n",
    "    # Salva as imagens\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join(output_dir, f\"img_{i}.jpg\")\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)\n",
    "        print(f\"Imagem salva em: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ea7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagem salva em: generated_samples/img_0.jpg\n",
      "Imagem salva em: generated_samples/img_1.jpg\n",
      "Imagem salva em: generated_samples/img_2.jpg\n",
      "Imagem salva em: generated_samples/img_3.jpg\n"
     ]
    }
   ],
   "source": [
    "generate(\n",
    "    mmgpt=vl_gpt,\n",
    "    vl_chat_processor=vl_chat_processor,\n",
    "    prompt=prompt,\n",
    "    temperature=1.0,\n",
    "    parallel_size=4, # Número de imagens a gerar\n",
    "    cfg_weight=5.0,\n",
    "    image_token_num_per_image=576,\n",
    "    img_size=384,\n",
    "    patch_size=16,\n",
    "    output_dir=\"generated_samples\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
